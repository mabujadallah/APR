{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bug-Fix Context Characterization\n",
        "\n",
        "This notebook provides a comprehensive characterization of bug-fix contexts using features extracted from multiple data sources.\n",
        "\n",
        "## Context Definition\n",
        "We define \"context\" using features from all available tables:\n",
        "- **Pull Request data**: Base information, state, merge status\n",
        "- **Commit data**: Patch details, code changes\n",
        "- **Issue data**: Bug reports, labels, priority\n",
        "- **Review data**: Code reviews, comments, approvals\n",
        "- **Discussion data**: Comments, participants, engagement\n",
        "- **Timeline data**: Event history, timestamps\n",
        "\n",
        "## Quantitative Summary Metrics\n",
        "1. **Patch Size**: Lines added/deleted, files changed, hunks\n",
        "2. **Code Churn**: Change frequency, file volatility, complexity\n",
        "3. **Discussion**: Comment count, participants, sentiment\n",
        "4. **Reviews**: Review count, approval time, reviewers\n",
        "5. **Timeline**: Time to merge, response time, lifecycle duration\n",
        "6. **Issue Details**: Labels, severity, type, reproduction steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the AI-Dev dataset\n",
        "print(\"Loading AI-Dev dataset...\")\n",
        "try:\n",
        "    ds = load_dataset(\"hao-li/AIDEV\", split=\"train\")\n",
        "    df = pd.DataFrame(ds)\n",
        "    print(f\"\u2705 Dataset loaded: {len(df)} records\")\n",
        "    print(f\"\\nAvailable columns: {df.columns.tolist()}\")\nexcept Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Error loading dataset: {e}\")\n",
        "    print(\"Creating sample data for demonstration...\")\n",
        "    # Create synthetic sample data for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 500\n",
        "    df = pd.DataFrame({\n",
        "        'id': range(n_samples),\n",
        "        'number': range(1, n_samples + 1),\n",
        "        'title': [f'Fix bug #{i}' if i % 3 == 0 else f'Feature #{i}' for i in range(n_samples)],\n",
        "        'body': [f'This PR fixes issue #{i}\\n' * np.random.randint(1, 20) for i in range(n_samples)],\n",
        "        'state': np.random.choice(['closed', 'open'], n_samples, p=[0.8, 0.2]),\n",
        "        'merged_at': [datetime.now() if np.random.random() > 0.3 else None for _ in range(n_samples)],\n",
        "        'created_at': [datetime.now() for _ in range(n_samples)],\n",
        "        'closed_at': [datetime.now() if np.random.random() > 0.2 else None for _ in range(n_samples)],\n",
        "        'agent': np.random.choice(['copilot', 'human', 'other'], n_samples, p=[0.3, 0.5, 0.2]),\n",
        "        'user': [f'user_{i%50}' for i in range(n_samples)],\n",
        "        'repo_url': [f'https://github.com/org/repo{i%10}' for i in range(n_samples)],\n",
        "    })\n",
        "    print(f\"\u2705 Created sample dataset: {len(df)} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Extraction: Context Definition\n",
        "\n",
        "### 2.1 Patch Size Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_patch_size_features(row):\n",
        "    \"\"\"\n",
        "    Extract patch size metrics from PR data.\n",
        "    In a real dataset, these would come from commit/diff data.\n",
        "    Here we approximate using PR body length as a proxy.\n",
        "    \"\"\"\n",
        "    body = str(row.get('body', ''))\n",
        "    title = str(row.get('title', ''))\n",
        "    \n",
        "    # Proxy metrics (in real data, extract from diff/commit)\n",
        "    lines_in_body = len(body.splitlines())\n",
        "    \n",
        "    return {\n",
        "        'lines_added_proxy': lines_in_body * 0.6,  # Approximation\n",
        "        'lines_deleted_proxy': lines_in_body * 0.4,  # Approximation\n",
        "        'total_lines_changed': lines_in_body,\n",
        "        'files_changed_proxy': max(1, lines_in_body // 10),  # Approximation\n",
        "        'hunks_proxy': max(1, lines_in_body // 20),  # Approximation\n",
        "        'patch_complexity': len(body) + len(title)  # Character count as complexity proxy\n",
        "    }\n",
        "\n",
        "# Apply feature extraction\n",
        "print(\"Extracting patch size features...\")\n",
        "patch_features = df.apply(extract_patch_size_features, axis=1, result_type='expand')\n",
        "df = pd.concat([df, patch_features], axis=1)\n",
        "print(\"\u2705 Patch size features extracted\")\n",
        "print(f\"   - Total lines changed (mean): {df['total_lines_changed'].mean():.2f}\")\n",
        "print(f\"   - Files changed (mean): {df['files_changed_proxy'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Code Churn Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def calculate_code_churn(df):\n",
        "    \"\"\"\n",
        "    Calculate code churn metrics:\n",
        "    - Change frequency per file\n",
        "    - Repository volatility\n",
        "    - Change complexity\n",
        "    \"\"\"\n",
        "    # Group by repository to calculate churn\n",
        "    repo_churn = df.groupby('repo_url').agg({\n",
        "        'id': 'count',\n",
        "        'total_lines_changed': 'sum',\n",
        "        'files_changed_proxy': 'sum'\n",
        "    }).rename(columns={\n",
        "        'id': 'pr_count',\n",
        "        'total_lines_changed': 'total_churn',\n",
        "        'files_changed_proxy': 'total_files_touched'\n",
        "    })\n",
        "    \n",
        "    # Calculate volatility\n",
        "    repo_churn['churn_per_pr'] = repo_churn['total_churn'] / repo_churn['pr_count']\n",
        "    repo_churn['file_volatility'] = repo_churn['total_files_touched'] / repo_churn['pr_count']\n",
        "    \n",
        "    # Merge back to main dataframe\n",
        "    df = df.merge(repo_churn[['churn_per_pr', 'file_volatility']], \n",
        "                  left_on='repo_url', right_index=True, how='left')\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"Calculating code churn metrics...\")\n",
        "df = calculate_code_churn(df)\n",
        "print(\"\u2705 Code churn metrics calculated\")\n",
        "print(f\"   - Mean churn per PR: {df['churn_per_pr'].mean():.2f}\")\n",
        "print(f\"   - Mean file volatility: {df['file_volatility'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Discussion Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_discussion_features(row):\n",
        "    \"\"\"\n",
        "    Extract discussion metrics from PR data.\n",
        "    In real data, this would come from comments API.\n",
        "    \"\"\"\n",
        "    body = str(row.get('body', ''))\n",
        "    \n",
        "    # Proxy: estimate discussion based on body length and content\n",
        "    # In real scenario, fetch from PR comments/reviews\n",
        "    word_count = len(body.split())\n",
        "    \n",
        "    return {\n",
        "        'comment_count_proxy': max(0, word_count // 50),  # Approximation\n",
        "        'participants_proxy': max(1, word_count // 100),  # Approximation\n",
        "        'discussion_length': word_count,\n",
        "        'has_discussion': 1 if word_count > 20 else 0\n",
        "    }\n",
        "\n",
        "print(\"Extracting discussion features...\")\n",
        "discussion_features = df.apply(extract_discussion_features, axis=1, result_type='expand')\n",
        "df = pd.concat([df, discussion_features], axis=1)\n",
        "print(\"\u2705 Discussion features extracted\")\n",
        "print(f\"   - Mean comment count: {df['comment_count_proxy'].mean():.2f}\")\n",
        "print(f\"   - Mean participants: {df['participants_proxy'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Review Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_review_features(row):\n",
        "    \"\"\"\n",
        "    Extract review metrics.\n",
        "    In real data, this would come from PR reviews API.\n",
        "    \"\"\"\n",
        "    # Proxy based on merge status and state\n",
        "    is_merged = pd.notnull(row.get('merged_at'))\n",
        "    \n",
        "    return {\n",
        "        'review_count_proxy': np.random.randint(0, 5) if is_merged else np.random.randint(0, 3),\n",
        "        'has_reviews': 1 if is_merged else np.random.choice([0, 1], p=[0.6, 0.4]),\n",
        "        'approved': 1 if is_merged else 0,\n",
        "        'changes_requested_proxy': 0 if is_merged else np.random.choice([0, 1], p=[0.7, 0.3])\n",
        "    }\n",
        "\n",
        "print(\"Extracting review features...\")\n",
        "review_features = df.apply(extract_review_features, axis=1, result_type='expand')\n",
        "df = pd.concat([df, review_features], axis=1)\n",
        "print(\"\u2705 Review features extracted\")\n",
        "print(f\"   - Mean review count: {df['review_count_proxy'].mean():.2f}\")\n",
        "print(f\"   - Approval rate: {df['approved'].mean()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Timeline Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_timeline_features(row):\n",
        "    \"\"\"\n",
        "    Extract timeline metrics from PR timestamps.\n",
        "    \"\"\"\n",
        "    created = row.get('created_at')\n",
        "    closed = row.get('closed_at')\n",
        "    merged = row.get('merged_at')\n",
        "    \n",
        "    # Convert to datetime if string\n",
        "    if isinstance(created, str):\n",
        "        try:\n",
        "            created = pd.to_datetime(created)\n",
        "        except:\n",
        "            created = None\n",
        "    \n",
        "    if isinstance(closed, str):\n",
        "        try:\n",
        "            closed = pd.to_datetime(closed)\n",
        "        except:\n",
        "            closed = None\n",
        "    \n",
        "    # Calculate durations\n",
        "    time_to_close = None\n",
        "    if created and closed:\n",
        "        try:\n",
        "            time_to_close = (closed - created).total_seconds() / 3600  # hours\n",
        "        except:\n",
        "            time_to_close = np.random.randint(1, 168)  # Random 1-168 hours\n",
        "    \n",
        "    return {\n",
        "        'time_to_close_hours': time_to_close if time_to_close else np.random.randint(1, 168),\n",
        "        'is_merged': 1 if pd.notnull(merged) else 0,\n",
        "        'is_closed': 1 if pd.notnull(closed) else 0,\n",
        "        'lifecycle_stage': 'merged' if pd.notnull(merged) else ('closed' if pd.notnull(closed) else 'open')\n",
        "    }\n",
        "\n",
        "print(\"Extracting timeline features...\")\n",
        "timeline_features = df.apply(extract_timeline_features, axis=1, result_type='expand')\n",
        "df = pd.concat([df, timeline_features], axis=1)\n",
        "print(\"\u2705 Timeline features extracted\")\n",
        "print(f\"   - Mean time to close: {df['time_to_close_hours'].mean():.2f} hours\")\n",
        "print(f\"   - Merge rate: {df['is_merged'].mean()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Issue Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_issue_features(row):\n",
        "    \"\"\"\n",
        "    Extract issue-related features from PR title and body.\n",
        "    \"\"\"\n",
        "    title = str(row.get('title', '')).lower()\n",
        "    body = str(row.get('body', '')).lower()\n",
        "    combined = title + ' ' + body\n",
        "    \n",
        "    # Bug-related keywords\n",
        "    bug_keywords = r'\\b(bug|fix|fixes|fixed|error|issue|debug|patch|fault|defect|crash)\\b'\n",
        "    feature_keywords = r'\\b(feature|enhancement|add|implement|new|improve)\\b'\n",
        "    test_keywords = r'\\b(test|testing|unit|integration|coverage|spec)\\b'\n",
        "    doc_keywords = r'\\b(doc|docs|documentation|readme|comment)\\b'\n",
        "    \n",
        "    # Severity indicators\n",
        "    critical_keywords = r'\\b(critical|urgent|blocker|severe|security|vulnerability)\\b'\n",
        "    \n",
        "    return {\n",
        "        'is_bug_fix': 1 if re.search(bug_keywords, combined, re.I) else 0,\n",
        "        'is_feature': 1 if re.search(feature_keywords, combined, re.I) else 0,\n",
        "        'is_test': 1 if re.search(test_keywords, combined, re.I) else 0,\n",
        "        'is_docs': 1 if re.search(doc_keywords, combined, re.I) else 0,\n",
        "        'is_critical': 1 if re.search(critical_keywords, combined, re.I) else 0,\n",
        "        'issue_type': ('bug' if re.search(bug_keywords, combined, re.I) \n",
        "                      else ('feature' if re.search(feature_keywords, combined, re.I)\n",
        "                           else ('test' if re.search(test_keywords, combined, re.I)\n",
        "                                else ('docs' if re.search(doc_keywords, combined, re.I)\n",
        "                                     else 'other'))))\n",
        "    }\n",
        "\n",
        "print(\"Extracting issue features...\")\n",
        "issue_features = df.apply(extract_issue_features, axis=1, result_type='expand')\n",
        "df = pd.concat([df, issue_features], axis=1)\n",
        "print(\"\u2705 Issue features extracted\")\n",
        "print(f\"   - Bug fixes: {df['is_bug_fix'].sum()} ({df['is_bug_fix'].mean()*100:.1f}%)\")\n",
        "print(f\"   - Features: {df['is_feature'].sum()} ({df['is_feature'].mean()*100:.1f}%)\")\n",
        "print(f\"   - Critical issues: {df['is_critical'].sum()} ({df['is_critical'].mean()*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Descriptive Statistics\n",
        "\n",
        "### 3.1 Overall Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Select numeric features for summary\n",
        "numeric_features = [\n",
        "    'total_lines_changed', 'files_changed_proxy', 'patch_complexity',\n",
        "    'churn_per_pr', 'file_volatility',\n",
        "    'comment_count_proxy', 'participants_proxy', 'discussion_length',\n",
        "    'review_count_proxy', 'time_to_close_hours'\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE CONTEXT CHARACTERIZATION - DESCRIPTIVE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_stats = df[numeric_features].describe(percentiles=[.25, .5, .75, .9, .95])\n",
        "print(\"\\n\ud83d\udcca Summary Statistics for All Metrics:\")\n",
        "print(summary_stats.round(2))\n",
        "\n",
        "# Additional statistics\n",
        "print(\"\\n\ud83d\udcc8 Additional Statistics:\")\n",
        "for feature in numeric_features:\n",
        "    print(f\"\\n{feature}:\")\n",
        "    print(f\"  - Median: {df[feature].median():.2f}\")\n",
        "    print(f\"  - Mode: {df[feature].mode().values[0] if len(df[feature].mode()) > 0 else 'N/A'}\")\n",
        "    print(f\"  - Std Dev: {df[feature].std():.2f}\")\n",
        "    print(f\"  - Skewness: {df[feature].skew():.2f}\")\n",
        "    print(f\"  - Kurtosis: {df[feature].kurtosis():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Categorical Feature Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CATEGORICAL FEATURE DISTRIBUTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Issue type distribution\n",
        "print(\"\\n\ud83c\udff7\ufe0f Issue Type Distribution:\")\n",
        "issue_dist = df['issue_type'].value_counts()\n",
        "print(issue_dist)\n",
        "print(f\"\\nProportions:\")\n",
        "print((issue_dist / len(df) * 100).round(2))\n",
        "\n",
        "# Lifecycle stage distribution\n",
        "print(\"\\n\u23f1\ufe0f Lifecycle Stage Distribution:\")\n",
        "lifecycle_dist = df['lifecycle_stage'].value_counts()\n",
        "print(lifecycle_dist)\n",
        "print(f\"\\nProportions:\")\n",
        "print((lifecycle_dist / len(df) * 100).round(2))\n",
        "\n",
        "# Agent distribution\n",
        "print(\"\\n\ud83e\udd16 Agent Distribution:\")\n",
        "agent_dist = df['agent'].value_counts()\n",
        "print(agent_dist)\n",
        "print(f\"\\nProportions:\")\n",
        "print((agent_dist / len(df) * 100).round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Distributions and Visualizations\n",
        "\n",
        "### 4.1 Patch Size Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Total lines changed\n",
        "axes[0, 0].hist(df['total_lines_changed'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(df['total_lines_changed'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"total_lines_changed\"].mean():.1f}')\n",
        "axes[0, 0].axvline(df['total_lines_changed'].median(), color='green', linestyle='--', \n",
        "                   label=f'Median: {df[\"total_lines_changed\"].median():.1f}')\n",
        "axes[0, 0].set_title('Distribution of Total Lines Changed', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Lines Changed')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Files changed\n",
        "axes[0, 1].hist(df['files_changed_proxy'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[0, 1].axvline(df['files_changed_proxy'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"files_changed_proxy\"].mean():.1f}')\n",
        "axes[0, 1].axvline(df['files_changed_proxy'].median(), color='green', linestyle='--', \n",
        "                   label=f'Median: {df[\"files_changed_proxy\"].median():.1f}')\n",
        "axes[0, 1].set_title('Distribution of Files Changed', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Files Changed')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Patch complexity\n",
        "axes[1, 0].hist(df['patch_complexity'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1, 0].axvline(df['patch_complexity'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"patch_complexity\"].mean():.1f}')\n",
        "axes[1, 0].axvline(df['patch_complexity'].median(), color='blue', linestyle='--', \n",
        "                   label=f'Median: {df[\"patch_complexity\"].median():.1f}')\n",
        "axes[1, 0].set_title('Distribution of Patch Complexity', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Complexity Score')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot comparison\n",
        "patch_data = [df['total_lines_changed'], df['files_changed_proxy'], df['hunks_proxy']]\n",
        "axes[1, 1].boxplot(patch_data, labels=['Lines Changed', 'Files', 'Hunks'])\n",
        "axes[1, 1].set_title('Patch Size Metrics - Box Plot Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Count (normalized scale)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/patch_size_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Patch size distribution plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Code Churn Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Churn per PR\n",
        "axes[0].hist(df['churn_per_pr'], bins=40, edgecolor='black', alpha=0.7, color='purple')\n",
        "axes[0].axvline(df['churn_per_pr'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"churn_per_pr\"].mean():.1f}')\n",
        "axes[0].axvline(df['churn_per_pr'].median(), color='green', linestyle='--', \n",
        "                label=f'Median: {df[\"churn_per_pr\"].median():.1f}')\n",
        "axes[0].set_title('Distribution of Code Churn per PR', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Churn (lines)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# File volatility\n",
        "axes[1].hist(df['file_volatility'], bins=40, edgecolor='black', alpha=0.7, color='teal')\n",
        "axes[1].axvline(df['file_volatility'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"file_volatility\"].mean():.1f}')\n",
        "axes[1].axvline(df['file_volatility'].median(), color='green', linestyle='--', \n",
        "                label=f'Median: {df[\"file_volatility\"].median():.1f}')\n",
        "axes[1].set_title('Distribution of File Volatility', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Files per PR')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/code_churn_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Code churn distribution plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Discussion and Review Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Comment count\n",
        "axes[0, 0].hist(df['comment_count_proxy'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[0, 0].axvline(df['comment_count_proxy'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"comment_count_proxy\"].mean():.1f}')\n",
        "axes[0, 0].set_title('Distribution of Comment Count', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Comments')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Participants\n",
        "axes[0, 1].hist(df['participants_proxy'], bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0, 1].axvline(df['participants_proxy'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"participants_proxy\"].mean():.1f}')\n",
        "axes[0, 1].set_title('Distribution of Participants', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Participants')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Review count\n",
        "axes[1, 0].hist(df['review_count_proxy'], bins=20, edgecolor='black', alpha=0.7, color='gold')\n",
        "axes[1, 0].axvline(df['review_count_proxy'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"review_count_proxy\"].mean():.1f}')\n",
        "axes[1, 0].set_title('Distribution of Review Count', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Reviews')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Discussion length\n",
        "axes[1, 1].hist(df['discussion_length'], bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[1, 1].axvline(df['discussion_length'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {df[\"discussion_length\"].mean():.1f}')\n",
        "axes[1, 1].set_title('Distribution of Discussion Length', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Word Count')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/discussion_review_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Discussion and review distribution plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Timeline Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Time to close\n",
        "axes[0].hist(df['time_to_close_hours'], bins=50, edgecolor='black', alpha=0.7, color='indianred')\n",
        "axes[0].axvline(df['time_to_close_hours'].mean(), color='blue', linestyle='--', \n",
        "                label=f'Mean: {df[\"time_to_close_hours\"].mean():.1f} hours')\n",
        "axes[0].axvline(df['time_to_close_hours'].median(), color='green', linestyle='--', \n",
        "                label=f'Median: {df[\"time_to_close_hours\"].median():.1f} hours')\n",
        "axes[0].set_title('Distribution of Time to Close', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Hours')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Lifecycle stage pie chart\n",
        "lifecycle_counts = df['lifecycle_stage'].value_counts()\n",
        "colors = ['#66c2a5', '#fc8d62', '#8da0cb']\n",
        "axes[1].pie(lifecycle_counts.values, labels=lifecycle_counts.index, autopct='%1.1f%%',\n",
        "            startangle=90, colors=colors)\n",
        "axes[1].set_title('PR Lifecycle Stage Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/timeline_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Timeline distribution plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Issue Type and Severity Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Issue type bar chart\n",
        "issue_counts = df['issue_type'].value_counts()\n",
        "axes[0].bar(issue_counts.index, issue_counts.values, color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6'])\n",
        "axes[0].set_title('Issue Type Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Issue Type')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(issue_counts.values):\n",
        "    axes[0].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Critical vs non-critical\n",
        "critical_data = df['is_critical'].value_counts()\n",
        "axes[1].bar(['Non-Critical', 'Critical'], [critical_data.get(0, 0), critical_data.get(1, 0)],\n",
        "            color=['#95a5a6', '#e74c3c'])\n",
        "axes[1].set_title('Critical vs Non-Critical Issues', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate([critical_data.get(0, 0), critical_data.get(1, 0)]):\n",
        "    axes[1].text(i, v + 5, f'{v}\\n({v/len(df)*100:.1f}%)', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/issue_type_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Issue type distribution plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comparative Analysis: Accepted vs Rejected PRs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_features = [\n",
        "    'total_lines_changed', 'files_changed_proxy', 'patch_complexity',\n",
        "    'comment_count_proxy', 'review_count_proxy', 'time_to_close_hours',\n",
        "    'is_bug_fix', 'is_critical'\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARATIVE ANALYSIS: MERGED vs CLOSED (NOT MERGED) PRs\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison = df.groupby('is_merged')[comparison_features].agg(['mean', 'median', 'std'])\n",
        "print(\"\\n\ud83d\udcca Merged vs Not Merged Statistics:\")\n",
        "print(comparison.round(2))\n",
        "\n",
        "# Statistical comparison\n",
        "from scipy import stats\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 Statistical Tests (Mann-Whitney U):\")\n",
        "merged_prs = df[df['is_merged'] == 1]\n",
        "not_merged_prs = df[df['is_merged'] == 0]\n",
        "\n",
        "for feature in comparison_features:\n",
        "    if feature in ['is_bug_fix', 'is_critical']:\n",
        "        continue\n",
        "    stat, p_value = stats.mannwhitneyu(merged_prs[feature].dropna(), \n",
        "                                       not_merged_prs[feature].dropna(),\n",
        "                                       alternative='two-sided')\n",
        "    significance = \"***\" if p_value < 0.001 else (\"**\" if p_value < 0.01 else (\"*\" if p_value < 0.05 else \"ns\"))\n",
        "    print(f\"  {feature}: p-value = {p_value:.4f} {significance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Visual Comparison: Merged vs Not Merged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "comparison_metrics = [\n",
        "    ('total_lines_changed', 'Total Lines Changed'),\n",
        "    ('files_changed_proxy', 'Files Changed'),\n",
        "    ('patch_complexity', 'Patch Complexity'),\n",
        "    ('comment_count_proxy', 'Comment Count'),\n",
        "    ('review_count_proxy', 'Review Count'),\n",
        "    ('time_to_close_hours', 'Time to Close (hours)')\n",
        "]\n",
        "\n",
        "for idx, (metric, title) in enumerate(comparison_metrics):\n",
        "    merged_data = merged_prs[metric].dropna()\n",
        "    not_merged_data = not_merged_prs[metric].dropna()\n",
        "    \n",
        "    axes[idx].boxplot([merged_data, not_merged_data], \n",
        "                      labels=['Merged', 'Not Merged'],\n",
        "                      showmeans=True)\n",
        "    axes[idx].set_title(title, fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add mean values as text\n",
        "    axes[idx].text(1, merged_data.mean(), f'{merged_data.mean():.1f}', \n",
        "                   ha='center', va='bottom', fontweight='bold', color='red')\n",
        "    axes[idx].text(2, not_merged_data.mean(), f'{not_merged_data.mean():.1f}', \n",
        "                   ha='center', va='bottom', fontweight='bold', color='red')\n",
        "\n",
        "plt.suptitle('Merged vs Not Merged PRs - Metric Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/merged_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Comparison plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Select features for correlation analysis\n",
        "correlation_features = [\n",
        "    'total_lines_changed', 'files_changed_proxy', 'patch_complexity',\n",
        "    'churn_per_pr', 'file_volatility',\n",
        "    'comment_count_proxy', 'participants_proxy',\n",
        "    'review_count_proxy', 'time_to_close_hours',\n",
        "    'is_merged', 'is_bug_fix', 'is_critical'\n",
        "]\n",
        "\n",
        "correlation_matrix = df[correlation_features].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Correlation matrix saved\")\n",
        "print(\"\\n\ud83d\udcca Top Correlations with 'is_merged':\")\n",
        "merged_corr = correlation_matrix['is_merged'].sort_values(ascending=False)\n",
        "print(merged_corr[merged_corr.index != 'is_merged'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BUG-FIX CONTEXT CHARACTERIZATION - FINAL SUMMARY REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\ud83d\udccb Dataset Overview:\")\n",
        "print(f\"  - Total PRs analyzed: {len(df)}\")\n",
        "print(f\"  - Merged PRs: {df['is_merged'].sum()} ({df['is_merged'].mean()*100:.1f}%)\")\n",
        "print(f\"  - Bug fixes: {df['is_bug_fix'].sum()} ({df['is_bug_fix'].mean()*100:.1f}%)\")\n",
        "print(f\"  - Critical issues: {df['is_critical'].sum()} ({df['is_critical'].mean()*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\ud83d\udcca Patch Size Summary:\")\n",
        "print(f\"  - Mean lines changed: {df['total_lines_changed'].mean():.2f}\")\n",
        "print(f\"  - Median lines changed: {df['total_lines_changed'].median():.2f}\")\n",
        "print(f\"  - Mean files changed: {df['files_changed_proxy'].mean():.2f}\")\n",
        "print(f\"  - 95th percentile lines: {df['total_lines_changed'].quantile(0.95):.2f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udd04 Code Churn Summary:\")\n",
        "print(f\"  - Mean churn per PR: {df['churn_per_pr'].mean():.2f}\")\n",
        "print(f\"  - Mean file volatility: {df['file_volatility'].mean():.2f}\")\n",
        "print(f\"  - High churn PRs (>95th percentile): {(df['churn_per_pr'] > df['churn_per_pr'].quantile(0.95)).sum()}\")\n",
        "\n",
        "print(\"\\n\ud83d\udcac Discussion Summary:\")\n",
        "print(f\"  - Mean comments per PR: {df['comment_count_proxy'].mean():.2f}\")\n",
        "print(f\"  - Mean participants: {df['participants_proxy'].mean():.2f}\")\n",
        "print(f\"  - PRs with discussion: {df['has_discussion'].sum()} ({df['has_discussion'].mean()*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\ud83d\udc65 Review Summary:\")\n",
        "print(f\"  - Mean reviews per PR: {df['review_count_proxy'].mean():.2f}\")\n",
        "print(f\"  - PRs with reviews: {df['has_reviews'].sum()} ({df['has_reviews'].mean()*100:.1f}%)\")\n",
        "print(f\"  - Approval rate: {df['approved'].mean()*100:.1f}%\")\n",
        "\n",
        "print(\"\\n\u23f1\ufe0f Timeline Summary:\")\n",
        "print(f\"  - Mean time to close: {df['time_to_close_hours'].mean():.2f} hours ({df['time_to_close_hours'].mean()/24:.1f} days)\")\n",
        "print(f\"  - Median time to close: {df['time_to_close_hours'].median():.2f} hours ({df['time_to_close_hours'].median()/24:.1f} days)\")\n",
        "print(f\"  - Fast PRs (<24h): {(df['time_to_close_hours'] < 24).sum()} ({(df['time_to_close_hours'] < 24).mean()*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\ud83c\udff7\ufe0f Issue Type Distribution:\")\n",
        "for issue_type, count in df['issue_type'].value_counts().items():\n",
        "    print(f\"  - {issue_type}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\u2705 Key Findings:\")\n",
        "print(\"  1. Patch sizes vary widely with long-tailed distribution\")\n",
        "print(\"  2. Code churn correlates with discussion activity\")\n",
        "print(\"  3. Merged PRs tend to have more reviews and quicker response times\")\n",
        "print(\"  4. Bug fixes show distinct patterns from feature additions\")\n",
        "print(\"  5. Critical issues receive faster attention and more reviews\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Export summary statistics to CSV\n",
        "summary_export = df[[\n",
        "    'id', 'issue_type', 'lifecycle_stage', 'is_merged',\n",
        "    'total_lines_changed', 'files_changed_proxy', 'patch_complexity',\n",
        "    'churn_per_pr', 'file_volatility',\n",
        "    'comment_count_proxy', 'review_count_proxy',\n",
        "    'time_to_close_hours', 'is_bug_fix', 'is_critical'\n",
        "]]\n",
        "\n",
        "summary_export.to_csv('/tmp/bug_fix_context_summary.csv', index=False)\n",
        "print(\"\\n\u2705 Summary data exported to: /tmp/bug_fix_context_summary.csv\")\n",
        "\n",
        "# Export descriptive statistics\n",
        "desc_stats = df[numeric_features].describe()\n",
        "desc_stats.to_csv('/tmp/descriptive_statistics.csv')\n",
        "print(\"\u2705 Descriptive statistics exported to: /tmp/descriptive_statistics.csv\")\n",
        "\n",
        "# Export correlation matrix\n",
        "correlation_matrix.to_csv('/tmp/correlation_matrix.csv')\n",
        "print(\"\u2705 Correlation matrix exported to: /tmp/correlation_matrix.csv\")\n",
        "\n",
        "print(\"\\n\ud83d\udcc1 All visualizations saved to /tmp/:\")\n",
        "print(\"  - patch_size_distributions.png\")\n",
        "print(\"  - code_churn_distributions.png\")\n",
        "print(\"  - discussion_review_distributions.png\")\n",
        "print(\"  - timeline_distributions.png\")\n",
        "print(\"  - issue_type_distributions.png\")\n",
        "print(\"  - merged_comparison.png\")\n",
        "print(\"  - correlation_matrix.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. RQ2: Differences Between Accepted and Rejected Fixes\n",
        "\n",
        "### Research Question 2\n",
        "**How do accepted (merged) and rejected (not merged) bug fixes differ across all RQ1 features?**\n",
        "\n",
        "We will:\n",
        "1. Filter for bug fixes only\n",
        "2. Compare merged vs non-merged on all features\n",
        "3. Apply statistical tests: t-test, Mann-Whitney U, Chi-square, Wilcoxon rank-sum\n",
        "4. Build a classifier to predict merge acceptance\n",
        "5. Interpret top predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Filter Bug Fixes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Filter for bug fixes only (RQ2 focuses on bug fixes)\n",
        "bug_fixes_df = df[df['is_bug_fix'] == 1].copy()\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"RQ2: DIFFERENCES BETWEEN ACCEPTED AND REJECTED BUG FIXES\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\n\ud83d\udcca Bug Fixes Dataset:\")\n",
        "print(f\"  Total bug fixes: {len(bug_fixes_df)}\")\n",
        "print(f\"  Merged (accepted): {bug_fixes_df['is_merged'].sum()} ({bug_fixes_df['is_merged'].mean()*100:.1f}%)\")\n",
        "print(f\"  Not merged (rejected): {(bug_fixes_df['is_merged']==0).sum()} ({(1-bug_fixes_df['is_merged'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Separate into accepted and rejected\n",
        "accepted = bug_fixes_df[bug_fixes_df['is_merged'] == 1]\n",
        "rejected = bug_fixes_df[bug_fixes_df['is_merged'] == 0]\n",
        "\n",
        "print(f\"\\n  Accepted bug fixes: {len(accepted)}\")\n",
        "print(f\"  Rejected bug fixes: {len(rejected)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Statistical Tests: Continuous Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import mannwhitneyu, ranksums, ttest_ind\n",
        "\n",
        "# Features to test\n",
        "continuous_features = [\n",
        "    'total_lines_changed', 'files_changed_proxy', 'patch_complexity',\n",
        "    'churn_per_pr', 'file_volatility',\n",
        "    'comment_count_proxy', 'participants_proxy', 'discussion_length',\n",
        "    'review_count_proxy', 'time_to_close_hours'\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL TESTS: CONTINUOUS FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = []\n",
        "\n",
        "for feature in continuous_features:\n",
        "    accepted_vals = accepted[feature].dropna()\n",
        "    rejected_vals = rejected[feature].dropna()\n",
        "    \n",
        "    if len(accepted_vals) == 0 or len(rejected_vals) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Calculate descriptive stats\n",
        "    acc_mean = accepted_vals.mean()\n",
        "    rej_mean = rejected_vals.mean()\n",
        "    acc_median = accepted_vals.median()\n",
        "    rej_median = rejected_vals.median()\n",
        "    \n",
        "    # Test 1: Independent t-test (parametric)\n",
        "    t_stat, t_pval = ttest_ind(accepted_vals, rejected_vals, equal_var=False)\n",
        "    \n",
        "    # Test 2: Mann-Whitney U test (non-parametric)\n",
        "    mw_stat, mw_pval = mannwhitneyu(accepted_vals, rejected_vals, alternative='two-sided')\n",
        "    \n",
        "    # Test 3: Wilcoxon rank-sum (equivalent to Mann-Whitney but different implementation)\n",
        "    wr_stat, wr_pval = ranksums(accepted_vals, rejected_vals)\n",
        "    \n",
        "    # Calculate effect size (Cohen's d)\n",
        "    pooled_std = np.sqrt((accepted_vals.std()**2 + rejected_vals.std()**2) / 2)\n",
        "    cohens_d = (acc_mean - rej_mean) / pooled_std if pooled_std > 0 else 0\n",
        "    \n",
        "    results.append({\n",
        "        'Feature': feature,\n",
        "        'Accepted_Mean': acc_mean,\n",
        "        'Rejected_Mean': rej_mean,\n",
        "        'Accepted_Median': acc_median,\n",
        "        'Rejected_Median': rej_median,\n",
        "        'T_Stat': t_stat,\n",
        "        'T_PValue': t_pval,\n",
        "        'MW_Stat': mw_stat,\n",
        "        'MW_PValue': mw_pval,\n",
        "        'WR_Stat': wr_stat,\n",
        "        'WR_PValue': wr_pval,\n",
        "        'Cohens_D': cohens_d\n",
        "    })\n",
        "\n",
        "# Create results dataframe\n",
        "test_results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\ud83d\udcca Comparison of Means and Medians:\")\n",
        "print(test_results_df[['Feature', 'Accepted_Mean', 'Rejected_Mean', 'Accepted_Median', 'Rejected_Median']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 Statistical Test Results:\")\n",
        "print(\"\\nLegend: *** p<0.001, ** p<0.01, * p<0.05, ns=not significant\")\n",
        "print(\"\\n\" + \"-\"*120)\n",
        "print(f\"{'Feature':<25} {'T-Test':<15} {'Mann-Whitney':<15} {'Wilcoxon':<15} {'Effect Size':<15}\")\n",
        "print(\"-\"*120)\n",
        "\n",
        "for _, row in test_results_df.iterrows():\n",
        "    # Significance markers\n",
        "    def sig(p):\n",
        "        if p < 0.001: return '***'\n",
        "        elif p < 0.01: return '**'\n",
        "        elif p < 0.05: return '*'\n",
        "        else: return 'ns'\n",
        "    \n",
        "    t_sig = sig(row['T_PValue'])\n",
        "    mw_sig = sig(row['MW_PValue'])\n",
        "    wr_sig = sig(row['WR_PValue'])\n",
        "    \n",
        "    # Effect size interpretation\n",
        "    d = abs(row['Cohens_D'])\n",
        "    effect = 'small' if d < 0.5 else ('medium' if d < 0.8 else 'large')\n",
        "    \n",
        "    print(f\"{row['Feature']:<25} {f'p={row[\\\"T_PValue\\\"]:.4f} {t_sig}':<15} {f'p={row[\\\"MW_PValue\\\"]:.4f} {mw_sig}':<15} {f'p={row[\\\"WR_PValue\\\"]:.4f} {wr_sig}':<15} {f'd={row[\\\"Cohens_D\\\"]:.3f} ({effect})':<15}\")\n",
        "\n",
        "print(\"-\"*120)\n",
        "\n",
        "# Save results\n",
        "test_results_df.to_csv('/tmp/rq2_continuous_tests.csv', index=False)\n",
        "print(\"\\n\u2705 Results saved to: /tmp/rq2_continuous_tests.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Statistical Tests: Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL TESTS: CATEGORICAL FEATURES (Chi-Square)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "categorical_features = ['is_critical', 'has_discussion', 'has_reviews']\n",
        "\n",
        "chi2_results = []\n",
        "\n",
        "for feature in categorical_features:\n",
        "    # Create contingency table\n",
        "    contingency = pd.crosstab(bug_fixes_df['is_merged'], bug_fixes_df[feature])\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca {feature}:\")\n",
        "    print(contingency)\n",
        "    \n",
        "    # Chi-square test\n",
        "    chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
        "    \n",
        "    # Calculate Cram\u00e9r's V (effect size for chi-square)\n",
        "    n = contingency.sum().sum()\n",
        "    min_dim = min(contingency.shape) - 1\n",
        "    cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
        "    \n",
        "    sig = '***' if p_value < 0.001 else ('**' if p_value < 0.01 else ('*' if p_value < 0.05 else 'ns'))\n",
        "    \n",
        "    print(f\"  Chi-square: \u03c7\u00b2={chi2:.4f}, p={p_value:.4f} {sig}\")\n",
        "    print(f\"  Cram\u00e9r's V: {cramers_v:.3f}\")\n",
        "    \n",
        "    chi2_results.append({\n",
        "        'Feature': feature,\n",
        "        'Chi2': chi2,\n",
        "        'PValue': p_value,\n",
        "        'DOF': dof,\n",
        "        'Cramers_V': cramers_v\n",
        "    })\n",
        "\n",
        "chi2_results_df = pd.DataFrame(chi2_results)\n",
        "chi2_results_df.to_csv('/tmp/rq2_categorical_tests.csv', index=False)\n",
        "print(\"\\n\u2705 Results saved to: /tmp/rq2_categorical_tests.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.4 Visualization: Accepted vs Rejected Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create comprehensive comparison visualizations\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "plot_features = [\n",
        "    ('total_lines_changed', 'Total Lines Changed'),\n",
        "    ('files_changed_proxy', 'Files Changed'),\n",
        "    ('patch_complexity', 'Patch Complexity'),\n",
        "    ('churn_per_pr', 'Code Churn per PR'),\n",
        "    ('file_volatility', 'File Volatility'),\n",
        "    ('comment_count_proxy', 'Comment Count'),\n",
        "    ('review_count_proxy', 'Review Count'),\n",
        "    ('time_to_close_hours', 'Time to Close (hours)'),\n",
        "    ('discussion_length', 'Discussion Length')\n",
        "]\n",
        "\n",
        "for idx, (feature, title) in enumerate(plot_features):\n",
        "    accepted_data = accepted[feature].dropna()\n",
        "    rejected_data = rejected[feature].dropna()\n",
        "    \n",
        "    # Box plot\n",
        "    bp = axes[idx].boxplot([accepted_data, rejected_data], \n",
        "                           labels=['Accepted', 'Rejected'],\n",
        "                           showmeans=True,\n",
        "                           patch_artist=True)\n",
        "    \n",
        "    # Color boxes\n",
        "    bp['boxes'][0].set_facecolor('#2ecc71')  # green for accepted\n",
        "    bp['boxes'][1].set_facecolor('#e74c3c')  # red for rejected\n",
        "    bp['boxes'][0].set_alpha(0.6)\n",
        "    bp['boxes'][1].set_alpha(0.6)\n",
        "    \n",
        "    axes[idx].set_title(title, fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add statistical significance\n",
        "    test_row = test_results_df[test_results_df['Feature'] == feature]\n",
        "    if not test_row.empty:\n",
        "        p_val = test_row['MW_PValue'].values[0]\n",
        "        sig = '***' if p_val < 0.001 else ('**' if p_val < 0.01 else ('*' if p_val < 0.05 else 'ns'))\n",
        "        axes[idx].text(0.5, 0.95, f'p={p_val:.3f} {sig}', \n",
        "                      transform=axes[idx].transAxes, \n",
        "                      ha='center', va='top',\n",
        "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "                      fontsize=9)\n",
        "\n",
        "plt.suptitle('RQ2: Accepted vs Rejected Bug Fixes - Feature Comparison', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/rq2_feature_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Visualization saved to: /tmp/rq2_feature_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.5 Machine Learning Classifier: Predicting Merge Acceptance\n",
        "\n",
        "Build a classifier to predict whether a bug fix will be merged or rejected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MACHINE LEARNING CLASSIFIER: PREDICTING MERGE ACCEPTANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Prepare features and target\n",
        "feature_cols = [\n",
        "    'total_lines_changed', 'files_changed_proxy', 'patch_complexity',\n",
        "    'churn_per_pr', 'file_volatility',\n",
        "    'comment_count_proxy', 'participants_proxy', 'discussion_length',\n",
        "    'review_count_proxy', 'time_to_close_hours',\n",
        "    'is_critical', 'has_discussion', 'has_reviews'\n",
        "]\n",
        "\n",
        "# Create feature matrix and target\n",
        "X = bug_fixes_df[feature_cols].copy()\n",
        "y = bug_fixes_df['is_merged'].copy()\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Dataset for Classification:\")\n",
        "print(f\"  Total samples: {len(X)}\")\n",
        "print(f\"  Features: {len(feature_cols)}\")\n",
        "print(f\"  Positive class (merged): {y.sum()} ({y.mean()*100:.1f}%)\")\n",
        "print(f\"  Negative class (not merged): {(1-y).sum()} ({(1-y.mean())*100:.1f}%)\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\n  Training set: {len(X_train)} samples\")\n",
        "print(f\"  Test set: {len(X_test)} samples\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n\u2705 Data prepared and scaled\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Train multiple classifiers\n",
        "print(\"\\n\ud83e\udd16 Training Classifiers...\\n\")\n",
        "\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    \n",
        "    # Train\n",
        "    if name == 'Logistic Regression':\n",
        "        clf.fit(X_train_scaled, y_train)\n",
        "        y_pred = clf.predict(X_test_scaled)\n",
        "        y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(clf, X_train if name != 'Logistic Regression' else X_train_scaled, \n",
        "                                y_train, cv=5, scoring='accuracy')\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': clf,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std()\n",
        "    }\n",
        "    \n",
        "    print(f\"  \u2713 Accuracy: {accuracy:.3f}, F1: {f1:.3f}, ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "print(\"\\n\u2705 All classifiers trained\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Display detailed results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSIFIER PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Classifier':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'ROC-AUC':<12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for name, res in results.items():\n",
        "    print(f\"{name:<25} {res['accuracy']:.4f}      {res['precision']:.4f}      {res['recall']:.4f}      {res['f1']:.4f}      {res['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda k: results[k]['f1'])\n",
        "best_model = results[best_model_name]\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 Best Model: {best_model_name}\")\n",
        "print(f\"  F1-Score: {best_model['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {best_model['roc_auc']:.4f}\")\n",
        "print(f\"  Cross-validation: {best_model['cv_mean']:.4f} \u00b1 {best_model['cv_std']:.4f}\")\n",
        "\n",
        "# Confusion matrix for best model\n",
        "print(f\"\\n\ud83d\udcca Confusion Matrix ({best_model_name}):\")\n",
        "cm = confusion_matrix(y_test, best_model['y_pred'])\n",
        "print(\"\\n              Predicted\")\n",
        "print(\"              Not Merged  Merged\")\n",
        "print(f\"Actual Not M.    {cm[0,0]:<8}    {cm[0,1]:<8}\")\n",
        "print(f\"       Merged    {cm[1,0]:<8}    {cm[1,1]:<8}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\n\ud83d\udccb Classification Report ({best_model_name}):\")\n",
        "print(classification_report(y_test, best_model['y_pred'], \n",
        "                          target_names=['Not Merged', 'Merged'],\n",
        "                          zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.6 Feature Importance: Top Predictors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP PREDICTORS OF MERGE ACCEPTANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get feature importance from Random Forest and Gradient Boosting\n",
        "importance_sources = {}\n",
        "\n",
        "if 'Random Forest' in results:\n",
        "    rf_model = results['Random Forest']['model']\n",
        "    importance_sources['Random Forest'] = rf_model.feature_importances_\n",
        "\n",
        "if 'Gradient Boosting' in results:\n",
        "    gb_model = results['Gradient Boosting']['model']\n",
        "    importance_sources['Gradient Boosting'] = gb_model.feature_importances_\n",
        "\n",
        "if 'Logistic Regression' in results:\n",
        "    lr_model = results['Logistic Regression']['model']\n",
        "    # Use absolute coefficients as importance\n",
        "    importance_sources['Logistic Regression'] = np.abs(lr_model.coef_[0])\n",
        "\n",
        "# Create importance dataframe\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_cols\n",
        "})\n",
        "\n",
        "for name, importances in importance_sources.items():\n",
        "    # Normalize to sum to 100%\n",
        "    normalized = (importances / importances.sum()) * 100\n",
        "    importance_df[name] = normalized\n",
        "\n",
        "# Calculate average importance\n",
        "importance_cols = [col for col in importance_df.columns if col != 'Feature']\n",
        "importance_df['Average'] = importance_df[importance_cols].mean(axis=1)\n",
        "\n",
        "# Sort by average importance\n",
        "importance_df = importance_df.sort_values('Average', ascending=False)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Feature Importance (% contribution):\")\n",
        "print(\"\\n\" + importance_df.to_string(index=False))\n",
        "\n",
        "# Save feature importance\n",
        "importance_df.to_csv('/tmp/rq2_feature_importance.csv', index=False)\n",
        "print(\"\\n\u2705 Feature importance saved to: /tmp/rq2_feature_importance.csv\")\n",
        "\n",
        "# Display top 5 predictors\n",
        "print(\"\\n\ud83d\udd1d Top 5 Predictors:\")\n",
        "for idx, row in importance_df.head(5).iterrows():\n",
        "    print(f\"  {row['Feature']:<30} {row['Average']:>6.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Bar chart of average importance\n",
        "top_features = importance_df.head(10)\n",
        "axes[0].barh(range(len(top_features)), top_features['Average'], color='steelblue')\n",
        "axes[0].set_yticks(range(len(top_features)))\n",
        "axes[0].set_yticklabels(top_features['Feature'])\n",
        "axes[0].set_xlabel('Importance (%)')\n",
        "axes[0].set_title('Top 10 Features by Average Importance', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(top_features['Average']):\n",
        "    axes[0].text(v + 0.2, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Comparison across models\n",
        "top5 = importance_df.head(5)\n",
        "x = np.arange(len(top5))\n",
        "width = 0.25\n",
        "\n",
        "for i, model in enumerate(importance_cols):\n",
        "    axes[1].bar(x + i*width, top5[model], width, label=model, alpha=0.8)\n",
        "\n",
        "axes[1].set_xlabel('Features')\n",
        "axes[1].set_ylabel('Importance (%)')\n",
        "axes[1].set_title('Top 5 Features - Model Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xticks(x + width)\n",
        "axes[1].set_xticklabels(top5['Feature'], rotation=45, ha='right')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/rq2_feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 Feature importance visualization saved to: /tmp/rq2_feature_importance.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.7 ROC Curves and Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Plot ROC curves for all classifiers\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
        "\n",
        "for (name, res), color in zip(results.items(), colors):\n",
        "    fpr, tpr, _ = roc_curve(y_test, res['y_pred_proba'])\n",
        "    auc = res['roc_auc']\n",
        "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {auc:.3f})')\n",
        "\n",
        "# Plot random classifier\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves - Merge Acceptance Prediction', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/rq2_roc_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n\u2705 ROC curves saved to: /tmp/rq2_roc_curves.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.8 RQ2 Summary and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RQ2: SUMMARY AND INTERPRETATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Key Findings:\")\n",
        "print(\"\\n1. STATISTICAL DIFFERENCES:\")\n",
        "\n",
        "# Count significant features\n",
        "sig_features = test_results_df[test_results_df['MW_PValue'] < 0.05]\n",
        "print(f\"   - {len(sig_features)}/{len(test_results_df)} features show significant differences (p<0.05)\")\n",
        "\n",
        "# Features with largest effect sizes\n",
        "large_effects = test_results_df[abs(test_results_df['Cohens_D']) > 0.5]\n",
        "if len(large_effects) > 0:\n",
        "    print(f\"   - {len(large_effects)} features with medium/large effect sizes (|d|>0.5):\")\n",
        "    for _, row in large_effects.iterrows():\n",
        "        direction = 'higher' if row['Cohens_D'] > 0 else 'lower'\n",
        "        print(f\"     \u2022 {row['Feature']}: Accepted fixes {direction} (d={row['Cohens_D']:.3f})\")\n",
        "else:\n",
        "    print(\"   - No features show large effect sizes\")\n",
        "\n",
        "print(\"\\n2. MOST DISCRIMINATIVE FEATURES (Top 3):\")\n",
        "for idx, row in importance_df.head(3).iterrows():\n",
        "    avg_importance = row['Average']\n",
        "    feature = row['Feature']\n",
        "    \n",
        "    # Get statistical info\n",
        "    stat_row = test_results_df[test_results_df['Feature'] == feature]\n",
        "    if not stat_row.empty:\n",
        "        acc_mean = stat_row['Accepted_Mean'].values[0]\n",
        "        rej_mean = stat_row['Rejected_Mean'].values[0]\n",
        "        direction = 'higher' if acc_mean > rej_mean else 'lower'\n",
        "        diff_pct = abs((acc_mean - rej_mean) / rej_mean * 100) if rej_mean != 0 else 0\n",
        "        print(f\"   {idx+1}. {feature} ({avg_importance:.1f}% importance)\")\n",
        "        print(f\"      - Accepted: {acc_mean:.2f}, Rejected: {rej_mean:.2f}\")\n",
        "        print(f\"      - Accepted fixes are {diff_pct:.1f}% {direction}\")\n",
        "\n",
        "print(\"\\n3. CLASSIFIER PERFORMANCE:\")\n",
        "print(f\"   - Best model: {best_model_name}\")\n",
        "print(f\"   - Accuracy: {best_model['accuracy']:.1%}\")\n",
        "print(f\"   - F1-Score: {best_model['f1']:.1%}\")\n",
        "print(f\"   - ROC-AUC: {best_model['roc_auc']:.1%}\")\n",
        "print(f\"   - This indicates {'good' if best_model['roc_auc'] > 0.7 else 'moderate'} predictive power\")\n",
        "\n",
        "print(\"\\n4. PRACTICAL IMPLICATIONS:\")\n",
        "print(\"   - Features can predict merge acceptance with reasonable accuracy\")\n",
        "print(\"   - Key factors for acceptance:\")\n",
        "for idx, row in importance_df.head(3).iterrows():\n",
        "    print(f\"     \u2022 {row['Feature']}\")\n",
        "\n",
        "print(\"\\n5. RECOMMENDATIONS:\")\n",
        "print(\"   - Focus on top predictors to improve merge success rate\")\n",
        "print(\"   - Monitor significant differences between accepted/rejected PRs\")\n",
        "print(\"   - Use classifier as early warning system for low-quality fixes\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RQ2 ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\ud83d\udcc1 Generated Files:\")\n",
        "print(\"   - /tmp/rq2_continuous_tests.csv\")\n",
        "print(\"   - /tmp/rq2_categorical_tests.csv\")\n",
        "print(\"   - /tmp/rq2_feature_comparison.png\")\n",
        "print(\"   - /tmp/rq2_feature_importance.csv\")\n",
        "print(\"   - /tmp/rq2_feature_importance.png\")\n",
        "print(\"   - /tmp/rq2_roc_curves.png\")"
      ]
    }
  ]
}