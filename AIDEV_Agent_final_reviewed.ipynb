{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# AIDEV_Agent_final_reviewed\n","\n","This notebook is an improved, reviewed version of AIDEV_Agent_final.ipynb. Improvements and fixes: \n","- Use the dataset 'agent' column to detect Copilot-related PRs instead of searching repo_url.\n","- Vectorized and faster labeling of accepted/rejected PRs.\n","- Safer handling of missing title/body/agent values.\n","- Use a more robust regex for bug keyword detection.\n","- Use PR body lines as a clearly-documented proxy for context size, treating empty bodies as 0.\n","- Stratified sampling to preserve label ratios when downsampling.\n","- Additional aggregated stats (counts, mean, median) and clearer plots.\n","\n","Assumptions and notes:\n","- The original dataset does not include the diff or files changed in the current fields used, so \"lines changed\" remains an explicit proxy (PR body lines). If diffs or files_changed fields exist, swap to those.\n","- \"Copilot\" detection is best-effort using the 'agent' field in the dataset; if you have other signals (commit messages, comments, repo file lists), use them instead.\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# Imports\n","from datasets import load_dataset\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 1. Load dataset (train split)\n","print(\"Loading AI-Dev dataset...\")\n","ds = load_dataset(\"hao-li/AIDev\", split=\"train\")\n","df = pd.DataFrame(ds)\n","print(\"Loaded rows:\", len(df))\n","print(\"Columns available:\", df.columns.tolist())\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 2. Keep only relevant columns (guard missing columns)\n","cols = [\n","    \\"id\\", \\"number\\", \\"title\\", \\"user\\", \\"state\\", \\"created_at\\", \\"closed_at\\",\n","    \\"merged_at\\", \\"repo_url\\", \\"html_url\\", \\"body\\", \\"agent\\"\n","]\n","existing = [c for c in cols if c in df.columns]\n","df = df[existing].copy()\n","\n","# 3. Safe fills for string fields\n","for c in [\"body\", \"title\", \"agent\"]:\n","    if c in df.columns:\n","        df[c] = df[c].fillna(\"").astype(str)\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 4. Vectorized labeling: merged -> accepted (1), closed (not merged) -> rejected (0)\n","df['label'] = np.nan\n","if 'merged_at' in df.columns:\n","    df.loc[df['merged_at'].notnull(), 'label'] = 1\n","# closed but not merged -> rejected\n","if 'state' in df.columns and 'merged_at' in df.columns:\n","    closed_mask = (df['merged_at'].isnull()) & (df['state'].fillna('').str.lower() == 'closed')\n","    df.loc[closed_mask, 'label'] = 0\n","# Drop PRs without label info\n","valid_df = df.dropna(subset=['label']).copy()\n","valid_df['label'] = valid_df['label'].astype(int)\n","print(\"PRs with merge/close label:\", len(valid_df))\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 5. Approximate context size (explicit proxy): number of lines in PR body; empty body -> 0\n","def body_lines_count(s):\n","    s = str(s) if s is not None else ''\n","    s = s.strip()\n","    return len(s.splitlines()) if s else 0\n","\n","valid_df['n_lines_changed'] = valid_df['body'].apply(body_lines_count)\n","print(\"Mean lines (proxy):\", valid_df['n_lines_changed'].mean())\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 6. Robust bug/fix keyword detection using word boundaries\n","bug_pat = re.compile(r\"\\b(bug|fix|fixes|fixed|error|issue|debug|patch|fault)\\b\", flags=re.I)\n","valid_df['has_bug_kw_title'] = valid_df['title'].apply(lambda t: bool(bug_pat.search(t))) if 'title' in valid_df.columns else False\n","valid_df['has_bug_kw_body'] = valid_df['body'].apply(lambda b: bool(bug_pat.search(b)))\n","valid_df['has_bug_kw'] = (valid_df['has_bug_kw_title'] | valid_df['has_bug_kw_body']).astype(int)\n","print(\"PRs with bug/fix keywords (overall):\", int(valid_df['has_bug_kw'].sum()))\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 7. Detect Copilot-related PRs using the 'agent' column (preferred)\n","if 'agent' in valid_df.columns:\n","    valid_df['agent_norm'] = valid_df['agent'].str.lower()\n","    valid_df['has_copilot_agent'] = valid_df['agent_norm'].str.contains(r'copilot|copilotchat|copilotx|github copilot', na=False)\n","else:\n","    valid_df['has_copilot_agent'] = False\n","\n","print(\"Detected Copilot agent PRs:\", int(valid_df['has_copilot_agent'].sum()))\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 8. Filter Copilot PRs and optionally downsample with stratification to N samples (preserve label ratios)\n","copilot_df = valid_df[valid_df['has_copilot_agent'] == True].copy()\n","print(\"Total copilot PRs available:\", len(copilot_df))\n","\n","N = 300\n","if len(copilot_df) > N:\n","    counts = copilot_df['label'].value_counts().sort_index()\n","    # target per label proportional to observed counts\n","    target = (counts / counts.sum() * N).round().astype(int)\n","    # adjust rounding to sum to N\n","    diff = N - target.sum()\n","    if diff != 0:\n","        # add/subtract the diff to the largest group to balance rounding artifacts\n","        largest_label = target.idxmax()\n","        target[largest_label] = target[largest_label] + diff\n","    samples = []\n","    for lbl, n in target.items():\n","        available = counts.get(lbl, 0)\n","        n = int(min(n, available))\n","        samples.append(copilot_df[copilot_df['label'] == lbl].sample(n, random_state=42))\n","    copilot_df = pd.concat(samples).sample(frac=1, random_state=42).reset_index(drop=True)\n","    print(\"Downsampled to N=\", len(copilot_df))\n","else:\n","    print(\"Using all copilot PRs (<=N)\")\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 9. RQ1: Describe Copilot PR contexts (summary stats)\n","rq1_lines_mean = copilot_df['n_lines_changed'].mean() if len(copilot_df) else float('nan')\n","rq1_bug_kw_rate = copilot_df['has_bug_kw'].mean() * 100 if len(copilot_df) else float('nan')\n","print(\"===== RQ1: Context Characteristics =====\")\n","print(f\"Average lines changed per PR (approx.): {rq1_lines_mean:.2f}\")\n","print(f\"Bug/fix keywords present in {rq1_bug_kw_rate:.1f}% of PRs\")\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 10. RQ2: Compare Accepted vs Rejected Fixes\n","rq2_group = copilot_df.groupby('label').agg(\n","    n_prs=('id', 'count'),\n","    mean_lines=('n_lines_changed', 'mean'),\n","    median_lines=('n_lines_changed', 'median'),\n","    bug_kw_rate=('has_bug_kw', 'mean')\n",")\n","print(\"===== RQ2: Accepted vs Rejected Fixes =====\")\n","print(rq2_group)\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# 11. Visualizations: boxplot + bar chart for bug keyword rates\n","if len(copilot_df):\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","    # Boxplot of lines by label\n","    copilot_df.boxplot(column='n_lines_changed', by='label', grid=False, ax=ax[0])\n","    ax[0].set_title('Approx. Lines Changed â€” Accepted (1) vs Rejected (0)')\n","    ax[0].set_xlabel('Label (0=Rejected, 1=Accepted)')\n","    ax[0].set_ylabel('Approx. Context Size (lines in PR body)')\n","    # Bar chart for bug keyword rate\n","    rates = copilot_df.groupby('label')['has_bug_kw'].mean() * 100\n","    rates.plot(kind='bar', ax=ax[1])\n","    ax[1].set_title('Bug/Fixed Keyword Rate by Label (%)')\n","    ax[1].set_xlabel('Label')\n","    ax[1].set_ylabel('Percent with bug/fix keywords')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No copilot PRs to visualize.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Next steps / recommendations:\n","- If you can augment the dataset with file diffs or a files_changed count, replace the PR body line proxy with those fields.\n","- If agent is unreliable, consider scanning commit metadata, PR comments, or repository file lists for .github/copilot config files (requires additional API calls).\n","- Run statistical tests (e.g., Mann-Whitney U) to check whether differences in context size between accepted/rejected are significant.\n"]}]}